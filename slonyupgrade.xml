<?xml version="1.0" encoding="UTF-8"?>
<!-- Dernière modification
     le       $Date$
     par      $Author$
     révision $Revision$ -->

<sect1 id="slonyupgrade">
<title> &slony1; Upgrade </title>
<indexterm><primary>upgrading &slony1; to a newer version</primary></indexterm>

<para> When upgrading &slony1;, the installation on all nodes in a
cluster must be upgraded at once, using the &lslonik;
command <xref linkend="stmtupdatefunctions"/>.</para>

<para> While this requires temporarily stopping replication, it does
not forcibly require an outage for applications that submit
updates. </para>

<para>The proper upgrade procedure is thus:</para>
<itemizedlist>
<listitem><para> Stop the &lslon; processes on all nodes.
(<emphasis>e.g.</emphasis> - old version of &lslon;)</para></listitem>
<listitem><para> Install the new version of &lslon; software on all
nodes.</para></listitem>
<listitem><para> Execute a &lslonik; script containing the
command <command>update functions (id = [whatever]);</command> for
each node in the cluster.</para>
<note><para>Remember that your slonik upgrade script like all other 
slonik scripts must contain the proper preamble commands to function.
</para></note></listitem>
<listitem><para> Start all slons.  </para> </listitem>
</itemizedlist>

<para> The overall operation is relatively safe: If there is any
mismatch between component versions, the &lslon; will refuse to start
up, which provides protection against corruption. </para>

<para> You need to be sure that the C library containing SPI trigger
functions has been copied into place in the &postgres; build.  There
are multiple possible approaches to this:</para>

<para> The trickiest part of this is ensuring that the C library
containing SPI functions is copied into place in the &postgres; build;
the easiest and safest way to handle this is to have two separate
&postgres; builds, one for each &slony1; version, where the postmaster
is shut down and then restarted against the <quote>new</quote> build;
that approach requires a brief database outage on each node.</para>

<para> While that approach has been found to be easier and safer,
nothing prevents one from carefully copying &slony1; components for
the new version into place to overwrite the old version as
the <quote>install</quote> step.  That might <emphasis>not</emphasis>
work on <trademark>Windows</trademark> if it locks library files that
are in use.</para>

<variablelist>

<varlistentry><term>Run <command>make install</command> to install new
&slony1; components on top of the old</term>

<listitem><para>If you build &slony1; on the same system on which it
is to be deployed, and build from sources, overwriting the old with
the new is as easy as <command>make install</command>.  There is no
need to restart a database backend; just to stop &lslon; processes,
run the <command>UPDATE FUNCTIONS</command> script, and start new
&lslon; processes.</para>

<para> Unfortunately, this approach requires having a build
environment on the same host as the deployment.  That may not be
consistent with efforts to use common &postgres; and &slony1; binaries
across a set of nodes. </para>
</listitem></varlistentry>

<varlistentry><term>Create a new &postgres; and &slony1; build</term>

<listitem><para>With this approach, the old &postgres; build with old
&slony1; components persists after switching to a new &postgres; build
with new &slony1; components. In order to switch to the new &slony1;
build, you need to restart the
&postgres; <command>postmaster</command>, therefore interrupting
applications, in order to get it to be aware of the location of the
new components. </para></listitem></varlistentry>

</variablelist>
</sect1>
