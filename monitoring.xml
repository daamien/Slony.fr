<?xml version="1.0" encoding="UTF-8"?>
<!-- Dernière modification
     le       $Date$
     par      $Author$
     révision $Revision$ -->

<sect1 id="monitoring">
<title>Monitoring</title>

<indexterm><primary>monitoring &slony1;</primary></indexterm>

<sect2> <title> &nagios; Replication Checks </title>

<indexterm><primary>&nagios; for monitoring replication</primary></indexterm>

<para> The script in the <filename>tools</filename> directory called
<command> psql_replication_check.pl </command> represents some of the
best answers arrived at in attempts to build replication tests to plug
into the <ulink url="http://www.nagios.org/"> &nagios; </ulink> system
monitoring tool.</para>

<para> A former script, <filename>
test_slony_replication.pl</filename>, took a <quote>clever</quote>
approach where a <quote>test script</quote> is periodically run, which
rummages through the &slony1; configuration to find origin and
subscribers, injects a change, and watches for its propagation through
the system.  It had two problems:</para>
<itemizedlist>

<listitem><para> Connectivity problems to the
<emphasis>single</emphasis> host where the test ran would make it look
as though replication was destroyed.  Overall, this monitoring
approach has been fragile to numerous error conditions.</para>
</listitem>

<listitem><para> &nagios; has no ability to benefit from the
<quote>cleverness</quote> of automatically exploring the set of nodes.
You need to set up a &nagios; monitoring rule for each and every node
being monitored.  </para> </listitem>
</itemizedlist>

<para> The new script, <command>psql_replication_check.pl</command>,
takes the minimalist approach of assuming that the system is an online
system that sees regular <quote>traffic,</quote> so that you can
define a view specifically for the replication test called
<envar>replication_status</envar> which is expected to see regular
updates.  The view simply looks for the youngest
<quote>transaction</quote> on the node, and lists its timestamp, age,
and some bit of application information that might seem useful to see.
</para>

<itemizedlist>

<listitem><para> In an inventory system, that might be the order
number for the most recently processed order. </para> </listitem>

<listitem><para> In a domain registry, that might be the name of the
most recently created domain.</para> </listitem>

</itemizedlist>

<para> An instance of the script will need to be run for each node
that is to be monitored; that is the way &nagios; works. </para>

</sect2>

<sect2 id="slonymrtg"> <title> Monitoring &slony1; using MRTG </title>

<indexterm><primary>MRTG for monitoring replication</primary></indexterm>

<para> One user reported on the &slony1; mailing list how to configure
<ulink url="http://people.ee.ethz.ch/~oetiker/webtools/mrtg/">
<application> mrtg - Multi Router Traffic Grapher </application>
</ulink> to monitor &slony1; replication.</para>

<para> ... Since I use <application>mrtg</application> to graph data
from multiple servers I use snmp (<application>net-snmp</application>
to be exact).  On database server, I added the following line to
<application>snmpd</application> configuration:</para>

<programlisting>
exec replicationLagTime  /cvs/scripts/snmpReplicationLagTime.sh 2
where <filename> /cvs/scripts/snmpReplicationLagTime.sh</filename> looks like this:
</programlisting>


<programlisting>
#!/bin/bash
/home/pgdba/work/bin/psql -U pgdba -h 127.0.0.1 -p 5800 -d _DBNAME_ -qAt -c
"select cast(extract(epoch from st_lag_time) as int8) FROM _irr.sl_status
WHERE st_received = $1"
</programlisting>

<para> Then, in mrtg configuration,  add this target:</para>
<programlisting>
Target[db_replication_lagtime]:extOutput.3&amp;extOutput.3:public at db::30:::
MaxBytes[db_replication_lagtime]: 400000000
Title[db_replication_lagtime]: db: replication lag time
PageTop[db_replication_lagtime]: &lt;H1&gt;db: replication lag time&lt;/H1&gt;
Options[db_replication_lagtime]: gauge,nopercent,growright
</programlisting>


<para> Alternatively, Ismail Yenigul points out how he managed to
monitor slony using <application>MRTG</application> without installing
<application>SNMPD</application>.</para>

<para> Here is the mrtg configuration</para>

<programlisting>
Target[db_replication_lagtime]:`/bin/snmpReplicationLagTime.sh 2`
MaxBytes[db_replication_lagtime]: 400000000
Title[db_replication_lagtime]: db: replication lag time
PageTop[db_replication_lagtime]: &lt;H1&gt;db: replication lag time&lt;/H1&gt;
Options[db_replication_lagtime]: gauge,nopercent,growright
</programlisting>

<para> and here is the modified version of the script</para>

<programlisting>
# cat /bin/snmpReplicationLagTime.sh
#!/bin/bash

output=`/usr/bin/psql -U slony -h 192.168.1.1 -d endersysecm -qAt -c
"select cast(extract(epoch from st_lag_time) as int8) FROM _mycluster.sl_status WHERE st_received = $1"`
echo $output
echo $output
echo 
echo
# end of script#
</programlisting>


<note><para> MRTG expects four lines from the script, and since there
are only two lines provided, the output must be padded to four
lines. </para> </note>

</sect2>

<sect2 id="testslonystate"> <title> test_slony_state</title>

<indexterm><primary>script test_slony_state to test replication state</primary></indexterm>

<para> This script does various sorts of analysis of the state of a
&slony1; cluster.</para>

<para> You specify arguments including <option>database</option>,
<option>host</option>, <option>user</option>,
<option>cluster</option>, <option>password</option>, and
<option>port</option> to connect to any of the nodes on a cluster.
You also specify a <option>mailprog</option> command (which should be
a program equivalent to <productname>Unix</productname>
<application>mailx</application>) and a recipient of email. </para>

<para> You may alternatively specify database connection parameters
via the environment variables used by
<application>libpq</application>, <emphasis>e.g.</emphasis> - using
<envar>PGPORT</envar>, <envar>PGDATABASE</envar>,
<envar>PGUSER</envar>, <envar>PGSERVICE</envar>, and such.</para>

<para> The script then rummages through <xref linkend="table.sl-path"/>
to find all of the nodes in the cluster, and the DSNs to allow it to,
in turn, connect to each of them.</para>

<para> For each node, the script examines the state of things,
including such things as:

<itemizedlist>
<listitem><para> Checking <xref linkend="table.sl-listen"/> for some
<quote>analytically determinable</quote> problems.  It lists paths
that are not covered.</para></listitem>

<listitem><para> Providing a summary of events by origin node</para>

<para> If a node hasn't submitted any events in a while, that likely
suggests a problem.</para></listitem>

<listitem><para> Summarizes the <quote>aging</quote> of table <xref
linkend="table.sl-confirm"/> </para>

<para> If one or another of the nodes in the cluster hasn't reported
back recently, that tends to lead to cleanups of tables like <xref
linkend="table.sl-log-1"/> and <xref linkend="table.sl-seqlog"/> not
taking place.</para></listitem>

<listitem><para> Summarizes what transactions have been running for a
long time</para>

<para> This only works properly if the statistics collector is
configured to collect command strings, as controlled by the option
<option> stats_command_string = true </option> in <filename>
postgresql.conf </filename>.</para>

<para> If you have broken applications that hold connections open,
this will find them.</para>

<para> If you have broken applications that hold connections open,
that has several unsalutory effects as <link
linkend="longtxnsareevil"> described in the
FAQ</link>.</para></listitem>

</itemizedlist></para>

<para> The script does some diagnosis work based on parameters in the
script; if you don't like the values, pick your favorites!</para>

</sect2>

<sect2 id="search-logs"> <title> <command>search-logs.sh</command> </title>

<indexterm><primary> search &slony1; logs using search-logs.sh </primary></indexterm>

<para> This script is constructed to search for &slony1; log files at
a given path (<envar>LOGHOME</envar>), based both on the naming
conventions used by the <xref linkend="launchclusters"/> and <xref
linkend="slonwatchdog"/> systems used for launching &lslon; 
processes.</para>

<para> Errors, if found, are listed, by log file, and emailed to the
specified user (<envar>LOGRECIPIENT</envar>); if no email address is
specified, output goes to standard output. </para>

<para> <envar>LOGTIMESTAMP</envar> allows overriding what hour to
evaluate (rather than the last hour). </para>

<para> An administrator might run this script once an hour to monitor
for replication problems. </para>
</sect2>

<sect2 id="wikigen"> <title> Building MediaWiki Cluster Summary </title>

<indexterm><primary> generating Wiki documentation of a cluster </primary></indexterm>

<para> The script <filename>mkmediawiki.pl </filename>, in
<filename>tools</filename>, may be used to generate a cluster summary
compatible with the popular <ulink url="http://www.mediawiki.org/">
MediaWiki </ulink> software. Note that the
<option>--categories</option> permits the user to specify a set of
(comma-delimited) categories with which to associate the output.  If
you have a series of &slony1; clusters, passing in the option
<option>--categories=slony1</option> leads to the MediaWiki instance
generating a category page listing all &slony1; clusters so
categorized on the wiki.  </para>

<para> The gentle user might use the script as follows: </para>

<screen>
~/logtail.en>         mvs login -d mywiki.example.info -u "Chris Browne" -p `cat ~/.wikipass` -w wiki/index.php                     
Doing login with host: logtail and lang: en
~/logtail.en> perl $SLONYHOME/tools/mkmediawiki.pl --host localhost --database slonyregress1 --cluster slony_regress1 --categories=Slony-I  > Slony_replication.wiki
~/logtail.en> mvs commit -m "More sophisticated generated Slony-I cluster docs" Slony_replication.wiki
Doing commit Slony_replication.wiki with host: logtail and lang: en
</screen>

<para> Note that <command>mvs</command> is a client written in Perl;
on <ulink url="http://www.debian.org/"> Debian GNU/Linux</ulink>, the
relevant package is called
<application>libwww-mediawiki-client-perl</application>; other systems
may have a packaged version of this under some similar name. </para>

</sect2>
</sect1>
