<?xml version="1.0" encoding="UTF-8"?>
<!-- Dernière modification
     le       $Date$
     par      $Author$
     révision $Revision$ -->

<sect1 id="adminscripts">
<title>Scripts d'administration</title>

<indexterm><primary>scripts d'administration de &slony1;</primary></indexterm>

<para> Un certain nombre de scripts ont été developpés tout au long de l'histoire de
	&slony1; pour aider les utilisateurs à gérer leurs clusters. Cette section ainsi
	que celle sur le <xref linkend="monitoring"/> et la  <xref
linkend="maintenance"/> décrit leur utilisation. </para>

<sect2 id="altperl"> <title>Les scripts altperl</title>

<indexterm><primary> Scriptd altperl pour &slony1;</primary></indexterm>

<para>Il existe un ensemble de scripts qui simplifie l'administration de plusieurs 
instances &slony1;. Ces scripts supportent une nombre variable de noeuds. Ils peuvent
être installés pendant le processus d'installation :</para>

<para><command>
 ./configure --with-perltools
</command></para>

<para>Ceci va produire un certain nombre de scripts avec le préfixe 
<command>slonik_</command>.  Ils éliminent les risques de confusion en se référençant
à un fichier central de configuration qui contient les détails de la configuration 
de votre site. Un exemple documenté de ce fichier est fourni dans 
<filename>altperl/slon_tools.conf-sample</filename>. La pluspart dispose également
une aide en ligne grace à l'option "--help", ce qui les rend plus facile à prendre en main
et à utiliser.
</para>

<para>La plupart des scripts de génération Slonik utilise la sortie STDOUT. 
Pendant un temps, les commandes étaient passées directement à <xref linkend="slonik"/>
pour qu'il les éxecute. Malheureusement, il s'agit d'une méthode trop . 
<quote>agressive</quote>, car de légères coquilles dans la ligne de commande 
peuvent conduire, dans certains cas, à des situations calamiteuses.
Tout administrateur qui se respecte doit relire le script 
<emphasis>avant</emphasis> de l'envoyer à <xref linkend="slonik"/>.</para>

<sect3><title>Gestion de multiples clusters</title>
<indexterm><primary>Gérer de multiples clusters avec les outils altperl</primary></indexterm>

<para>La variable d'environnement <envar>SLONYNODES</envar> est utilisée pour déterminer
quel fichier de configuration Perl sera utilisé pour controller les noeuds du cluster
&slony1;. Si elle n'est pas fournie le fichier  <filename>slon_tools.conf</filename> 
situé dans l'emplacement par défaut sera utilisé. </para>

<para>Voici la liste des variables qui peuvent être configurées :
<itemizedlist>

<listitem><para><envar>$CLUSTER_NAME</envar>=orglogs;	# Quel est le nom du cluster de replication ?</para></listitem>
<listitem><para><envar>$LOGDIR</envar>='/opt/OXRS/log/LOGDBS';	# Quel est le répertoire des logs ?</para></listitem>
<listitem><para><envar>$APACHE_ROTATOR</envar>="/opt/twcsds004/OXRS/apache/rotatelogs";  # Si ce paramètre est défini, il indique où trouver le gestionnaire de logs d'Apache</para></listitem>
<listitem><para><envar>foldCase</envar> # Si la valeur est 1, les noms d'objet ( y compris les noms de schéma) seront transformés en minuscule. Par défaut, les noms d'objets restent inchangés. Notons que &postgres; lui-même transforme les noms d'objets en minuscule;
Si vous créez une table avec la commande <command> CREATE TABLE
MA_TABLE (Id INTEGER, MoNChamp text);</command>, le résultat sera
équivalent à <command> create table ma_table (id integer,
monchamp text);</command>, le nom de la table et ses champs sera transformé en minuscule.
</para>

</listitem>
</itemizedlist>
</para>

<para> Vous pouvez ensuite définir un ensemble de noeuds qui participeront à la réplication
en utilisant plusieurs appels à la fonction <function>add_node()</function>.
</para>

<para><command>
  add_node (host => '10.20.30.40', dbname => 'orglogs', port => 5437,
			  user => 'postgres', node => 4, parent => 1);
</command></para>

<para>Les paramètres de la fonction <function>add_node()</function> sont les suivants :</para>

<programlisting>
my %PARAMS =   (host=> undef,		# nom de l'hôte
	   	dbname => 'template1',	# nom de la base
		port => 5432,		# numéro du port
		user => 'postgres',	# utilisateur de la connexion
		node => undef,		# numéro du noeud
		password => undef,	# mot de passe de l'utilisateur
		parent => 1,		# l'identifiant du noeud père
		noforward => undef	# ce noeud doit-il retransmettre les modifications ?
                sslmode => undef        # mode SSL - détermine la priorité 
                                        # d'utilisation de la couche SSL
                                        # = disable,allow,prefer,require
);
</programlisting>
</sect3> 

<sect3><title>Configuration d'un ensemble de réplication </title>
<indexterm><primary>cluster.set1 - configuration d'un ensemble de réplication pour les outils
Perl</primary></indexterm>

<para>La variable d'environnement UNIX <envar>SLONYSET</envar> est utilisée pout déterminer
quel fichier de configuration doit être lu pour connaître les objets qui sont contenus 
dans un ensemble donné.</para>

<para>Contrairement à <envar>SLONYNODES</envar>, qui est essentielle pour 
<emphasis>tous</emphasis> les scripts de génération <xref linkend="slonik"/>,
celle-ci n'est nécessaire que lorsqu'on exécute 
<filename>create_set</filename>, car il s'agit du seul script qui controlle
le placement des tables dans les différents ensemble de réplication.
</para>

</sect3>
<sect3><title>slonik_build_env</title>
<indexterm><primary>slonik_build_env</primary></indexterm>

<para>Cette commande interroge la base de données et produit un résultat qui 
peut être utilisé dans <filename>slon_tools.conf</filename>, notamment :</para>
<itemizedlist>

<listitem><para> une série d'appel à <function>add_node()</function> pour configurer 
le cluster</para></listitem>
<listitem><para> les tableaux <envar>@KEYEDTABLES</envar>,
<envar>@SERIALT</envar>, et <envar>@SEQUENCES</envar></para></listitem>
</itemizedlist>
</sect3>
<sect3><title>slonik_print_preamble</title>

<para>Cette commande produit simplement le <quote>preambule</quote> qui est nécessaire
à chaque scripts slonik. En fait, elle fournit un <quote>squelette</quote> de script slonik
qui ne fait pas d'action particulière.</para>
</sect3>
<sect3><title>slonik_create_set</title>

<para>Cette commande nécessite que les variables <envar>SLONYSET</envar> et 
<envar>SLONYNODES</envar> soit configurées;
Elle permet de produire le script <command>slonik</command> 
qui configure un ensemble de réplication en décrivant les tables et les séquences
qui seront répliquées.</para>
</sect3>
<sect3><title>slonik_drop_node</title>

<para>Cette commande produit une script Slonik qui supprime une noeud du cluster &slony1; .</para>
</sect3>
<sect3><title>slonik_drop_set</title>

<para>Cette commande produit un script Slonik qui supprime un ensemble de réplication
(<emphasis>c'est à dire</emphasis> un groupe de tables et de séquences).</para>
</sect3>

<sect3 id="slonik-drop-table"><title>slonik_drop_table</title>

<para>Cette commande produit un script Slonik qui supprime une table de la réplication.
Elle nécessite en entrée l'identifiant de la table ( disponible dans le table
<envar>sl_table</envar>). </para>
</sect3>

<sect3><title>slonik_execute_script</title>

<para>Cette commande produit un script pour effectuer des changements DDL sur un ensemble 
de réplication.</para>
</sect3>
<sect3><title>slonik_failover</title>

<para>Cette commande produit un script qui demande une bascule d'urgence entre un noeud
mort et une nouvelle origine</para>
</sect3>
<sect3><title>slonik_init_cluster</title>

<para>Cette commande produit un script Slonik qui intialise un cluster &slony1; tout entier,
y compris les noeuds, les voies de communications et les voies d'écoute.
</para>
</sect3>
<sect3><title>slonik_merge_sets</title>

<para>Cette commande produit un script Slonik qui fusionne deux ensembles de réplication.
</para>
</sect3>
<sect3><title>slonik_move_set</title>

<para>Cette commande produit un script Slonik qui déplace l'origine d'un ensemble de réplication vers un autre noeud.</para>
</sect3>
<sect3><title>replication_test</title>

<para>Ce script vérifie que &slony1; réplique correctement les données.</para>
</sect3>
<sect3><title>slonik_restart_node</title>

<para>Cette commande produit un script Slonik que demande le redémarrage d'un noeud.
Elle était particulièrement utile avant la version 1.0.5, lorsque les noeuds étaient
bloqués suite à la mort du démon slon.</para>
</sect3>
<sect3><title>slonik_restart_nodes</title>

<para>Cette commande produit un script Slonik qui redémarre tous les noeuds du cluster.
Elle n'est pas très utile.</para>
</sect3>
<sect3><title>slony_show_configuration</title>

<para>Cette commande affiche la configuration de l'environnement (c'est à dire la variable <envar>SLONYNODES</envar>).</para>
</sect3>
<sect3><title>slon_kill</title>

<para>Cette commande tue le chien de garde slony et tous les démons slon pour un
ensemble de réplication donné. Elle ne fonctionne que si ces processus fonctionnent sur l'hôte local, bien sûr !
</para>
</sect3>
<sect3><title>slon_start</title>

<para>Cette commande démarre le démon slon pour un cluster et un noeud donnés, elle utilise
le chien de garde slon_watchdog pour s'assurer qu'il fonctionne.</para>
</sect3>
<sect3 id="slonwatchdog"><title>slon_watchdog</title>

<para>Processus utilisé par <command>slon_start</command>.</para>

</sect3><sect3><title>slon_watchdog2</title>

<para>Ce script est un chien de garde plus malin; il surveille un noeud donné et relance 
le processus slon si il constate qu'aucune modification ne s'est produite depuis 20 minutes ou plus.</para>

<para>Ceci est utile lorsqu'une connexion réseau est instable et que le démon slon s'arrête sans crier gare.</para>

</sect3>
<sect3><title>slonik_store_node</title>

<para>Cette commande ajoute un noeud dans un cluster.</para>
</sect3>
<sect3><title>slonik_subscribe_set</title>

<para>Generates Slonik script to subscribe a particular node to a particular replication set.</para>

</sect3><sect3><title>slonik_uninstall_nodes</title>

<para>Cette commande parcours le cluster et supprime le schéma &slony1; sur tous les noeuds;
Vous pouvez utilisez cet outil si vous souhaitez détruire la réplication sur l'ensemble du cluster. Il s'agit d'un script <emphasis>TRÈS</emphasis> dangereux !</para>

</sect3><sect3><title>slonik_unsubscribe_set</title>

<para>Cette commande produit un script Slonik qui désabonne un noeud d'un ensemble de réplication.</para>

</sect3>
<sect3><title>slonik_update_nodes</title>

<para>Cette commande produit un script Slonik qui incite tous les noeuds à mettre à jour les fonctions &slony1;. Elle est utile lorsque l'on effectue un changement de version de &slony1;
.</para>
</sect3>
</sect2>

<sect2 id="mkslonconf"><title>mkslonconf.sh</title>

<indexterm><primary>Générer le fichier slon.conf pour &slony1;</primary></indexterm>

<para> Ce script shell est conçu parcourir un cluster &slony1; et 
produire un ensemble de fichiers <filename>slon.conf</filename> 
qui pourront être utilisé via l'option <command> slon -f slon.conf </command>.
</para>

<para> Lorsque toute la configuration est placée dans un fichier pour chaque &lslon;,
on peut alors facilement les invoquer, sans risquer d'oublier l'option
<command>-a</command>, ce qui peut provoquer le crash d'un noeud en mode
<link linkend="logshipping"> log shipping </link>. </para>

<para> Pour lancer le script, il faut configurer l'environnement de la manière suivante :
</para>

<itemizedlist>

<listitem><para> Tout d'abord, l'environnement doit être configuré avec les paramètres
adéquats pour que libpq puisse se connecter à une des bases de données du cluster.
Vous devez donc définir une combinaison parmi les variables d'environnement suivantes :
</para>

<itemizedlist>
<listitem><para><envar>PGPORT</envar></para></listitem>
<listitem><para><envar>PGDATABASE</envar></para></listitem>
<listitem><para><envar>PGHOST</envar></para></listitem>
<listitem><para><envar>PGUSER</envar></para><riab/listitem>
<listitem><para><envar>PGSERVICE</envar></para></listitem>
</itemizedlist>

</listitem>

<listitem><para> <envar>SLONYCLUSTER</envar> - le nom du cluster &slony1; qui 
doit être <quote>parcouru</quote>.  </para></listitem>

<listitem><para> <envar>MKDESTINATION</envar> - un répertoire qui accueillera
la configuration; le script crée un dossier
<filename>MKDESTINATION/$SLONYCLUSTER/conf</filename> pour les fichiers de configuration
des démons &lslon; et un dossier
<filename>MKDESTINATION/$SLONYCLUSTER/pid</filename> pour que &lslon; y stocke
les fichiers PID. </para></listitem>

<listitem><para> <envar>LOGHOME</envar> - un répertoire qui accueillera les fichiers
de log; un dossier nommé 
<command>$LOGHOME/$SLONYCLUSTER/node[numéro]</command> sera créé pour chaque noeud.
</para></listitem>

</itemizedlist>

<para> Pour chaque <quote>nouveau</quote> noeud qu'il découvre, ce script
va créer un nouveau fichier de configuration &lslon;. </para>

<warning><para> Il est important de préciser que ce script présente quelques particularités 
qu'il faut connaître, même aucune n'est vraiment surprenante.
</para>

<itemizedlist>

<listitem><para> Le DSN est positioné à la plus petite valeur trouvé pour
chaque noeud dans le table <envar>sl_path</envar>.  Vous devrez probablement
modifier cette valeur.</para></listitem>

<listitem><para> Plusieurs paramètres sont initialisés avec leur valeur par défaut;
Vous devrez probablement les réajuster à la main. </para></listitem>

<listitem><para> Si vous exécutez les processus &lslon; sur de multiples noeuds (<emphasis>par exemple</emphasis> - si vous utilisez &slony1; à travers un réseau WAN),
ce script va joyeusement créer des fichiers de configuration pour des
&lslon;s que vous comptez lancer sur un hôte différent.  </para>

<para> Vérifiez bien quels noeuds sont configurés avant de redémarrer les &lslon;s.  </para>

<para> En général, cela provoque des inconvénients mineurs,
comme, par exemple, un &lslon; fonctionnant de manière inapproprié,
 échouant suite à un problème de connectivité, (ce qui ne provoque pas
de dégats ! ), ou fonctionnant moins efficacement vu qu'il se trouve du mauvais
coté du <quote>tuyau</quote>.</para>

<para> D'un autre côté, si vous faites fonctionner un noeud en mode log shipping sur
le site distant, l'arrivée d'un &lslon; que that
<emphasis>ne collecte pas</emphasis> les logs peut ruiner une semaine complète d'activité. </para>
</listitem>
</itemizedlist>

</warning>

<para> Les fichiers produits par <filename>mkslonconf.sh</filename>
sont spécifiquement conçu pour gérer des &lslon;s sur de multiples clusters
avec le script décrit dans la section suivante... </para>

</sect2>

<sect2 id="launchclusters"><title> launch_clusters.sh </title>

<indexterm><primary>lancer un cluster &slony1; cluster en utilisant les fichiers slon.conf </primary></indexterm>

<para> Voici un autre script shell qui utilise la configuration produite
par <filename>mkslonconf.sh</filename> et qui peut être utilisé lors du 
démarrage du système, à la suite des processus <filename>rc.d</filename>,
ou dans un processus cron, pour s'assurer que les processus &lslon; 
fonctionnent.</para>

<para> Il utilise les variables d'environnement suivantes :</para>

<itemizedlist>

<listitem><para><envar>PATH</envar> qui doit contenir, de préférence au début, le
chemin vers les binaires &lslon; qui doivent être exécutés.</para></listitem>

<listitem><para><envar>SLHOME</envar> indique le répertoire
<quote>home</quote> qui contient les fichiers de configuration de &lslon;;
ces fichiers doivent être rangés en sous-repertoires, un pour chaque cluster,
avec un nom de fichier du type <filename>node1.conf</filename>,
<filename>node2.conf</filename>, et ainsi de suite. </para>

<para> Le script utilise la commande <command>find $SLHOME/$cluster/conf
-name "node[0-9]*.conf"</command> pour trouver les fichiers de configuration &lslon;.</para>

<para> Si vous déplacez ces fichiers, ou si vous les renommez, ils ne seront pas trouvés;
c'est une façon très simple de supprimer des noeuds.</para>
</listitem>

<listitem><para><envar>LOGHOME </envar> indicates the
<quote>home</quote> directory for log storage.</para>

<para> This script does not assume the use of the Apache log rotator
to manage logs; in that &postgres; version 8 does its own log
rotation, it seems undesirable to retain a dependancy on specific log
rotation <quote>technology.</quote> </para></listitem>

<listitem><para><envar>CLUSTERS</envar> is a list of &slony1; clusters
under management. </para></listitem>

</itemizedlist>

<para> In effect, you could run this every five minutes, and it would
launch any missing &lslon; processes. </para>
</sect2>

<sect2 id="extractschema"><title> <filename> slony1_extract_schema.sh </filename> </title>

<indexterm><primary>script - slony1_extract_schema.sh</primary></indexterm>

<para> You may find that you wish to create a new node some time well
after creating a cluster.  The script <filename>
slony1_extract_schema.sh </filename> will help you with this.</para>

<para> A command line might look like the following:</para>

<para><command> PGPORT=5881 PGHOST=master.int.example.info ./slony1_extract_schema.sh payroll payroll temppayroll </command> </para>

<para> It performs the following:</para>

<itemizedlist>
<listitem><para> It dumps the origin node's schema, including the data in the &slony1; cluster schema. </para>

<para> Note that the extra environment variables <envar>PGPORT</envar>
and <envar>PGHOST</envar> to indicate additional information about
where the database resides. </para></listitem>

<listitem><para> This data is loaded into the freshly created temporary database, <envar>temppayroll</envar> </para> </listitem>
<listitem><para> The table and sequence OIDs in &slony1; tables are corrected   to point to the temporary database's configuration.  </para> </listitem>
<listitem><para>  A slonik script is run to perform <xref linkend="stmtuninstallnode"/> on the temporary database.   This eliminates all the special &slony1; tables, schema, and removes &slony1; triggers from replicated tables. </para> </listitem>
<listitem><para> Finally, <application>pg_dump</application> is run against the temporary database, delivering a copy of the cleaned up schema to standard output. </para> </listitem>
</itemizedlist>

</sect2>
<sect2><title> slony-cluster-analysis </title>

<indexterm><primary>script - slony-cluster-analysis</primary></indexterm>

<para> If you are running a lot of replicated databases, where there
are numerous &slony1; clusters, it can get painful to track and
document this.  The following tools may be of some assistance in this.</para>

<para> <application>slony-cluster-analysis.sh</application> is a shell
script intended to provide some over-time analysis of the
configuration of a &slony1; cluster.  You pass in the usual
<application>libpq</application> environment variables
(<envar>PGHOST</envar>, <envar>PGPORT</envar>,
<envar>PGDATABASE</envar>, and such) to connect to a member of a
&slony1; cluster, and pass the name of the cluster as an argument.</para>

<para> The script then does the following:</para>
<itemizedlist>
<listitem><para> Runs a series of queries against the &slony1; tables to get lists of nodes, paths, sets, and tables. </para> </listitem>
<listitem><para> This is stowed in a temporary file in <filename>/tmp</filename> </para> </listitem>
<listitem><para> A comparison is done between the present configuration and the configuration the last time the tool was run.  If the configuration differs, an email of the difference (generated using <application>diff</application>) is sent to a configurable email address. </para> </listitem>
<listitem><para> If the configuration has changed, the old configuration file is renamed to indicate when the script noticed the change. </para></listitem>
<listitem><para> Ultimately, the current configuration is stowed in <envar>LOGDIR</envar> in a filename like <filename>cluster.last </filename> </para> </listitem>
</itemizedlist>

<para> There is a sample <quote>wrapper</quote> script,
<filename>slony-cluster-analysis-mass.sh</filename>, which sets things
up to point to a whole bunch of &slony1; clusters.</para>

<para> This should make it easier for a group of DBAs to keep track of
two things: </para>

<itemizedlist>

<listitem><para> Documenting the current state of system
configuration.  </para></listitem>

<listitem><para> Noticing when configuration
changes. </para></listitem>
</itemizedlist>

</sect2>

<sect2 id="configurereplication"> <title> Generating slonik scripts
using <filename>configure-replication.sh</filename> </title>

<indexterm><primary> generate slonik scripts for a cluster </primary></indexterm>

<para> The <filename>tools</filename> script
<filename>configure-replication.sh</filename> is intended to automate
generating slonik scripts to configure replication.  This script is
based on the configuration approach taken by the <xref
linkend="testbed"/>.</para>

<para> This script uses a number (possibly large, if your
configuration needs to be particularly complex) of environment
variables to determine the shape of the configuration of a cluster.
It uses default values extensively, and in many cases, relatively few
environment values need to be set in order to get a viable
configuration. </para>

<sect3><title>Global Values</title>

<para> There are some values that will be used universally across a
cluster: </para>

<variablelist>
<varlistentry><term><envar>  CLUSTER </envar></term>
<listitem><para> Name of Slony-I cluster</para></listitem></varlistentry>
<varlistentry><term><envar>  NUMNODES </envar></term>
<listitem><para> Number of nodes to set up</para></listitem></varlistentry>

<varlistentry><term><envar>  PGUSER </envar></term>
<listitem><para> name of PostgreSQL superuser controlling replication</para></listitem></varlistentry>
<varlistentry><term><envar>  PGPORT </envar></term>
<listitem><para> default port number</para></listitem></varlistentry>
<varlistentry><term><envar>  PGDATABASE </envar></term>
<listitem><para> default database name</para></listitem></varlistentry>

<varlistentry><term><envar>  TABLES </envar></term>
<listitem><para> a list of fully qualified table names (<emphasis>e.g.</emphasis> - complete with
           namespace, such as <command>public.my_table</command>)</para></listitem></varlistentry>
<varlistentry><term><envar>  SEQUENCES </envar></term>
<listitem><para> a list of fully qualified sequence names (<emphasis>e.g.</emphasis> - complete with
           namespace, such as <command>public.my_sequence</command>)</para></listitem></varlistentry>

</variablelist>

<para>Defaults are provided for <emphasis>all</emphasis> of these
values, so that if you run
<filename>configure-replication.sh</filename> without setting any
environment variables, you will get a set of slonik scripts.  They may
not correspond, of course, to any database you actually want to
use...</para>
</sect3>

<sect3><title>Node-Specific Values</title>

<para>For each node, there are also four environment variables; for node 1: </para>
<variablelist>
<varlistentry><term><envar>  DB1 </envar></term>
<listitem><para> database to connect to</para></listitem></varlistentry>
<varlistentry><term><envar>  USER1 </envar></term>
<listitem><para> superuser to connect as</para></listitem></varlistentry>
<varlistentry><term><envar>  PORT1 </envar></term>
<listitem><para> port</para></listitem></varlistentry>
<varlistentry><term><envar>  HOST1 </envar></term>
<listitem><para> host</para></listitem></varlistentry>
</variablelist>

<para> It is quite likely that <envar>DB*</envar>,
<envar>USER*</envar>, and <envar>PORT*</envar> should be drawn from
the global <envar>PGDATABASE</envar>, <envar>PGUSER</envar>, and
<envar>PGPORT</envar> values above; having the discipline of that sort
of uniformity is usually a good thing.</para>

<para> In contrast, <envar>HOST*</envar> values should be set
explicitly for <envar>HOST1</envar>, <envar>HOST2</envar>, ..., as you
don't get much benefit from the redundancy replication provides if all
your databases are on the same server!</para>

</sect3>

<sect3><title>Resulting slonik scripts</title>

<para> slonik config files are generated in a temp directory under
<filename>/tmp</filename>.  The usage is thus:</para>

<itemizedlist>

<listitem> <para><filename>preamble.slonik</filename> is a
<quote>preamble</quote> containing connection info used by the other
scripts.</para>

<para> Verify the info in this one closely; you may want to keep this
permanently to use with future maintenance you may want to do on the
cluster.</para></listitem>

<listitem><para> <filename>create_set.slonik</filename></para>

<para>This is the first script to run; it sets up the requested nodes
as being &slony1; nodes, adding in some &slony1;-specific config
tables and such.</para>

<para>You can/should start slon processes any time after this step has
run.</para></listitem>

<listitem><para><filename>  store_paths.slonik</filename></para>

<para> This is the second script to run; it indicates how the &lslon;s
should intercommunicate.  It assumes that all &lslon;s can talk to all
nodes, which may not be a valid assumption in a complexly-firewalled
environment.  If that assumption is untrue, you will need to modify
the script to fix the paths.</para></listitem>

<listitem><para><filename>create_set.slonik</filename></para>

<para> This sets up the replication set consisting of the whole bunch
of tables and sequences that make up your application's database
schema.</para>

<para> When you run this script, all that happens is that triggers are
added on the origin node (node #1) that start collecting updates;
replication won't start until #5...</para>

<para>There are two assumptions in this script that could be
invalidated by circumstances:</para>

<itemizedlist>
     <listitem><para> That all of the tables and sequences have been
     included.</para>

     <para> This becomes invalid if new tables get added to your
     schema and don't get added to the <envar>TABLES</envar>
     list.</para> </listitem>

     <listitem><para> That all tables have been defined with primary
     keys.</para>

     <para> Best practice is to always have and use true primary keys.
     If you have tables that require choosing a candidate primary key
     or that require creating a surrogate key using <xref
     linkend="stmttableaddkey"/>, you will have to modify this script
     by hand to accomodate that. </para></listitem>

</itemizedlist>
</listitem>

<listitem><para> <filename> subscribe_set_2.slonik </filename></para>

  <para> And 3, and 4, and 5, if you set the number of nodes
  higher... </para>

  <para> This is the step that <quote>fires up</quote>
  replication.</para>

  <para> The assumption that the script generator makes is that all
  the subscriber nodes will want to subscribe directly to the origin
  node.  If you plan to have <quote>sub-clusters,</quote> perhaps
  where there is something of a <quote>master</quote> location at each
  data centre, you may need to revise that.</para>

  <para> The slon processes really ought to be running by the time you
  attempt running this step.  To do otherwise would be rather
  foolish.</para> </listitem>
</itemizedlist>

</sect3>
</sect2>

<sect2 id="bsd-ports-profile"> <title> <filename> slon.in-profiles </filename> </title>
<subtitle> Apache-Style profiles for FreeBSD <filename>ports/databases/slony/*</filename> </subtitle>

<indexterm><primary> Apache-style profiles for FreeBSD </primary> <secondary>FreeBSD </secondary> </indexterm>

<para> In the <filename>tools</filename> area, <filename>slon.in-profiles</filename> is a
script that might be used to start up &lslon; instances at the time of
system startup.  It is designed to interact with the FreeBSD Ports
system.</para>

</sect2>


<sect2 id="duplicate-node"> <title> <filename> duplicate-node.sh </filename> </title>
<indexterm><primary> duplicating nodes </primary> </indexterm>
<para> In the <filename>tools</filename> area,
<filename>duplicate-node.sh</filename> is a script that may be used to
help create a new node that duplicates one of the ones in the
cluster. </para>

<para> The script expects the following parameters: </para>
<itemizedlist>
<listitem><para> Cluster name </para> </listitem>
<listitem><para> New node number </para> </listitem>
<listitem><para> Origin node </para> </listitem>
<listitem><para> Node being duplicated </para> </listitem>
<listitem><para> New node </para> </listitem>
</itemizedlist>

<para> For each of the nodes specified, the script offers flags to
specify <function>libpq</function>-style parameters for
<envar>PGHOST</envar>, <envar>PGPORT</envar>,
<envar>PGDATABASE</envar>, and <envar>PGUSER</envar>; it is expected
that <filename>.pgpass</filename> will be used for storage of
passwords, as is generally considered best practice. Those values may
inherit from the <function>libpq</function> environment variables, if
not set, which is useful when using this for testing.  When
<quote>used in anger,</quote> however, it is likely that nearly all of
the 14 available parameters should be used. </para>

<para> The script prepares files, normally in
<filename>/tmp</filename>, and will report the name of the directory
that it creates that contain SQL and &lslonik; scripts to set up the
new node. </para>

<itemizedlist>
<listitem><para> <filename> schema.sql </filename> </para> 
<para> This is drawn from the origin node, and contains the <quote>pristine</quote> database schema that must be applied first.</para></listitem>
<listitem><para> <filename> slonik.preamble </filename> </para> 

<para> This <quote>preamble</quote> is used by the subsequent set of slonik scripts. </para> </listitem>
<listitem><para> <filename> step1-storenode.slonik </filename> </para> 
<para> A &lslonik; script to set up the new node. </para> </listitem>
<listitem><para> <filename> step2-storepath.slonik </filename> </para> 
<para> A &lslonik; script to set up path communications between the provider node and the new node. </para> </listitem>
<listitem><para> <filename> step3-subscribe-sets.slonik </filename> </para> 
<para> A &lslonik; script to request subscriptions for all replications sets.</para> </listitem>
</itemizedlist>

<para> For testing purposes, this is sufficient to get a new node working.  The configuration may not necessarily reflect what is desired as a final state:</para>

<itemizedlist>
<listitem><para> Additional communications paths may be desirable in order to have redundancy. </para> </listitem>
<listitem><para> It is assumed, in the generated scripts, that the new node should support forwarding; that may not be true. </para> </listitem>
<listitem><para> It may be desirable later, after the subscription process is complete, to revise subscriptions. </para> </listitem>
</itemizedlist>

</sect2>
</sect1>

